{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bloom Filter Project\n",
    "\n",
    "This notebook demonstrates the complete implementation and analysis of a Bloom Filter. We will cover data generation, Bloom Filter implementation, hash functions, testing, benchmarking, and visualizing results.\n",
    "\n",
    "## Step 1: Data Generation\n",
    "\n",
    "First, we generate different types of datasets that will be used for testing our Bloom Filter. These include random strings, natural language words, DNA sequences, and email addresses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import random\n",
    "import string\n",
    "import os\n",
    "import unittest\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Data Generation\n",
    "\n",
    "# Define the number of elements in the datasets\n",
    "NUM_ELEMENTS = 10 ** 4\n",
    "\n",
    "# Generate random strings related to computer science\n",
    "def generate_random_strings(file_path, num_strings, length=10):\n",
    "    keywords = [\"algorithm\", \"binary\", \"compiler\", \"data\", \"encryption\", \"function\", \"hardware\", \"interface\", \"java\", \"kernel\", \"library\", \"memory\", \"network\", \"object\", \"protocol\", \"queue\", \"recursion\", \"syntax\", \"thread\", \"variable\"]\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _ in range(num_strings):\n",
    "            rand_str = ''.join(random.choices(keywords, k=1)) + ''.join(random.choices(string.ascii_lowercase, k=length - 1))\n",
    "            f.write(rand_str + '\\n')\n",
    "\n",
    "# Generate natural language words related to data science\n",
    "def generate_natural_language_words(file_path, num_words):\n",
    "    words = [\"data\", \"science\", \"machine\", \"learning\", \"model\", \"algorithm\", \"statistics\", \"analysis\", \"big\", \"data\", \"mining\", \"predictive\", \"analytics\", \"visualization\", \"clustering\", \"classification\", \"regression\", \"python\", \"r\", \"sql\"]\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _ in range(num_words):\n",
    "            f.write(random.choice(words) + '\\n')\n",
    "\n",
    "# Generate real-world DNA sequences\n",
    "def generate_dna_sequences(file_path, num_sequences, length=10):\n",
    "    nucleotides = ['A', 'T', 'C', 'G']\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _ in range(num_sequences):\n",
    "            seq = ''.join(random.choices(nucleotides, k=length))\n",
    "            f.write(seq + '\\n')\n",
    "\n",
    "# Generate email addresses with specific domains\n",
    "def generate_email_addresses(file_path, num_addresses):\n",
    "    domains = [\"hotmail.com\", \"outlook.com\", \"gmail.com\", \"uhasselt.be\", \"kuleuven.be\"]\n",
    "    with open(file_path, 'w') as f:\n",
    "        for _ in range(num_addresses):\n",
    "            user = ''.join(random.choices(string.ascii_lowercase, k=5))\n",
    "            domain = random.choice(domains)\n",
    "            f.write(user + \"@\" + domain + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    generate_random_strings(\"data/random_strings.txt\", NUM_ELEMENTS)\n",
    "    generate_natural_language_words(\"data/natural_language_words.txt\", NUM_ELEMENTS)\n",
    "    generate_dna_sequences(\"data/dna_sequences.txt\", NUM_ELEMENTS)\n",
    "    generate_email_addresses(\"data/email_addresses.txt\", NUM_ELEMENTS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Bloom Filter Implementation\n",
    "\n",
    "Next, we implement the Bloom Filter and associated hash functions. The Bloom Filter uses a bitset to store hashed values and multiple hash functions to minimize the chance of collisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bloom Filter Implementation\n",
    "\n",
    "class BitSet:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.bitset = [0] * ((size // 64) + 1)\n",
    "\n",
    "    def add(self, index):\n",
    "        self.bitset[index // 64] |= (1 << (index % 64))\n",
    "\n",
    "    def contains(self, index):\n",
    "        return (self.bitset[index // 64] & (1 << (index % 64))) != 0\n",
    "\n",
    "import hashlib\n",
    "import mmh3\n",
    "import cityhash\n",
    "\n",
    "class HashFunctions:\n",
    "    @staticmethod\n",
    "    def hash_int32_jenkins(key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode('utf-8')\n",
    "        hash_value = 0\n",
    "        for char in key:\n",
    "            hash_value += char\n",
    "            hash_value += (hash_value << 10)\n",
    "            hash_value ^= (hash_value >> 6)\n",
    "        hash_value += (hash_value << 3)\n",
    "        hash_value ^= (hash_value >> 11)\n",
    "        hash_value += (hash_value << 15)\n",
    "        return hash_value & 0xffffffff\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_int32_shift(key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode('utf-8')\n",
    "        hash_value = 0\n",
    "        for char in key:\n",
    "            hash_value = (hash_value << 5) - hash_value + char\n",
    "        return hash_value & 0xffffffff\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_murmur(key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode('utf-8')\n",
    "        return mmh3.hash(key)\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_city(key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode('utf-8')\n",
    "        return cityhash.CityHash32(key)\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_sha256(key):\n",
    "        if isinstance(key, str):\n",
    "            key = key.encode('utf-8')\n",
    "        return int(hashlib.sha256(key).hexdigest(), 16) % (1 << 32)\n",
    "\n",
    "    @staticmethod\n",
    "    def hash_string(item):\n",
    "        if isinstance(item, str):\n",
    "            item = item.encode('utf-8')\n",
    "        return int(hashlib.md5(item).hexdigest(), 16)\n",
    "\n",
    "class BloomFilter:\n",
    "    def __init__(self, capacity, error_rate=0.01, hash_function='jenkins'):\n",
    "        self.size = self._best_m(capacity, error_rate)\n",
    "        self.hash_function_count = self._best_k(capacity, error_rate)\n",
    "        self.bitset = BitSet(self.size)\n",
    "\n",
    "        if hash_function == 'murmur':\n",
    "            self.get_hash_primary = HashFunctions.hash_murmur\n",
    "            self.get_hash_secondary = HashFunctions.hash_murmur\n",
    "        elif hash_function == 'city':\n",
    "            self.get_hash_primary = HashFunctions.hash_city\n",
    "            self.get_hash_secondary = HashFunctions.hash_city\n",
    "        elif hash_function == 'sha256':\n",
    "            self.get_hash_primary = HashFunctions.hash_sha256\n",
    "            self.get_hash_secondary = HashFunctions.hash_sha256\n",
    "        else:\n",
    "            self.get_hash_primary = HashFunctions.hash_int32_jenkins\n",
    "            self.get_hash_secondary = HashFunctions.hash_int32_shift\n",
    "\n",
    "    def add(self, item):\n",
    "        item_hash = HashFunctions.hash_string(item)\n",
    "        item_hash_bytes = str(item_hash).encode('utf-8')\n",
    "        primary_hash = self.get_hash_primary(item_hash_bytes)\n",
    "        secondary_hash = self.get_hash_secondary(item_hash_bytes)\n",
    "\n",
    "        if primary_hash is None or secondary_hash is None:\n",
    "            raise ValueError(\"Hash functions returned None\")\n",
    "\n",
    "        for i in range(1, self.hash_function_count + 1):\n",
    "            combined_hash = (primary_hash + i * secondary_hash) % self.size\n",
    "            self.bitset.add(combined_hash)\n",
    "\n",
    "    def contains(self, item):\n",
    "        item_hash = HashFunctions.hash_string(item)\n",
    "        item_hash_bytes = str(item_hash).encode('utf-8')\n",
    "        primary_hash = self.get_hash_primary(item_hash_bytes)\n",
    "        secondary_hash = self.get_hash_secondary(item_hash_bytes)\n",
    "\n",
    "        if primary_hash is None or secondary_hash is None:\n",
    "            return False\n",
    "\n",
    "        for i in range(1, self.hash_function_count + 1):\n",
    "            combined_hash = (primary_hash + i * secondary_hash) % self.size\n",
    "            if not self.bitset.contains(combined_hash):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_m(capacity, error_rate):\n",
    "        import math\n",
    "        return int(-capacity * math.log(error_rate) / (math.log(2) ** 2))\n",
    "\n",
    "    @staticmethod\n",
    "    def _best_k(capacity, error_rate):\n",
    "        import math\n",
    "        return int((BloomFilter._best_m(capacity, error_rate) / capacity) * math.log(2))\n",
    "\n",
    "    @staticmethod\n",
    "    def error_rate(capacity, size, hash_function_count):\n",
    "        import math\n",
    "        return (1 - math.exp(-hash_function_count * capacity / size)) ** hash_function_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Testing\n",
    "\n",
    "We implement unit tests to verify the functionality and accuracy of our Bloom Filter implementation. These tests check basic operations, edge cases, and the false positive rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Bloom Filter\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestBloomFilter(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.capacity = 10 ** 5\n",
    "        self.error_rate = 0.01\n",
    "        self.data = [\"apple\", \"banana\", \"grape\", \"orange\", \"watermelon\"]\n",
    "\n",
    "    def test_add_and_contains(self):\n",
    "        bf = BloomFilter(self.capacity, self.error_rate)\n",
    "        for item in self.data:\n",
    "            bf.add(item)\n",
    "            self.assertTrue(bf.contains(item))\n",
    "\n",
    "    def test_false_positive_rate(self):\n",
    "        bf = BloomFilter(self.capacity, self.error_rate)\n",
    "        for item in self.data:\n",
    "            bf.add(item)\n",
    "\n",
    "        false_positives = 0\n",
    "        for _ in range(10 ** 5):\n",
    "            random_string = \"random\" + str(_)\n",
    "            if bf.contains(random_string):\n",
    "                false_positives += 1\n",
    "\n",
    "        false_positive_rate = false_positives / (10 ** 5)\n",
    "        self.assertLessEqual(false_positive_rate, self.error_rate)\n",
    "\n",
    "    def test_edge_cases(self):\n",
    "        bf = BloomFilter(self.capacity, self.error_rate)\n",
    "        bf.add(\"\")\n",
    "        self.assertTrue(bf.contains(\"\"))\n",
    "\n",
    "        special_chars = \"!@#$%^&*()\"\n",
    "        bf.add(special_chars)\n",
    "        self.assertTrue(bf.contains(special_chars))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Benchmarking\n",
    "\n",
    "We benchmark the performance of the Bloom Filter by comparing it with other data structures such as linear search, BST, and AVL tree. We measure metrics like add time, search time, and memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmarking Bloom Filter\n",
    "\n",
    "def load_data(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        return file.read().splitlines()\n",
    "\n",
    "def memory_usage_psutil():\n",
    "    process = psutil.Process()\n",
    "    mem = process.memory_info().rss / float(2 ** 20)\n",
    "    return mem\n",
    "\n",
    "def cpu_usage_psutil():\n",
    "    process = psutil.Process()\n",
    "    return process.cpu_percent(interval=1.0)\n",
    "\n",
    "def random_string(length=10):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))\n",
    "\n",
    "def benchmark_bloom_filter(data, capacity, error_rate, hash_function='jenkins'):\n",
    "    bf = BloomFilter(capacity, error_rate, hash_function)\n",
    "\n",
    "    add_times = []\n",
    "    check_times = []\n",
    "    false_positives = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        start_time = time.time()\n",
    "        bf.add(item)\n",
    "        add_time = time.time() - start_time\n",
    "        add_times.append(add_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        if bf.contains(random_string()):\n",
    "            false_positives += 1\n",
    "        check_time = time.time() - start_time\n",
    "        check_times.append(check_time)\n",
    "\n",
    "    false_positive_rate = false_positives / len(data)\n",
    "    actual_bit_usage = sum(bin(x).count('1') for x in bf.bitset.bitset)\n",
    "    compression_rate = actual_bit_usage / bf.bitset.size\n",
    "\n",
    "    return {\n",
    "        \"add_time\": add_times,\n",
    "        \"check_time\": check_times,\n",
    "        \"false_positive_rate\": [false_positive_rate] * len(data),\n",
    "        \"compression_rate\": compression_rate,\n",
    "        \"memory_usage\": memory_usage_psutil()\n",
    "    }\n",
    "\n",
    "def benchmark_linear_search(data):\n",
    "    add_times = []\n",
    "    search_times = []\n",
    "\n",
    "    elements_list = []\n",
    "\n",
    "    for i in range(1, len(data) + 1):\n",
    "        start_time = time.time()\n",
    "        elements_list.append(data[i - 1])\n",
    "        add_time = time.time() - start_time\n",
    "        add_times.append(add_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        elements_list.index(data[i - 1])\n",
    "        search_time = time.time() - start_time\n",
    "        search_times.append(search_time)\n",
    "\n",
    "    return {\n",
    "        \"add_time\": add_times,\n",
    "        \"search_time\": search_times,\n",
    "        \"memory_usage\": memory_usage_psutil()\n",
    "    }\n",
    "\n",
    "class BSTNode:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "def insert_bst_iterative(root, value):\n",
    "    if root is None:\n",
    "        return BSTNode(value)\n",
    "\n",
    "    current = root\n",
    "    while True:\n",
    "        if value < current.value:\n",
    "            if current.left is None:\n",
    "                current.left = BSTNode(value)\n",
    "                break\n",
    "            current = current.left\n",
    "        else:\n",
    "            if current.right is None:\n",
    "                current.right = BSTNode(value)\n",
    "                break\n",
    "            current = current.right\n",
    "    return root\n",
    "\n",
    "def search_bst(root, value):\n",
    "    if root is None or root.value == value:\n",
    "        return root\n",
    "    if value < root.value:\n",
    "        return search_bst(root.left, value)\n",
    "    return search_bst(root.right)\n",
    "\n",
    "def benchmark_bst(data):\n",
    "    random.shuffle(data)  # Randomize the data before insertion\n",
    "\n",
    "    add_times = []\n",
    "    search_times = []\n",
    "\n",
    "    root = None\n",
    "\n",
    "    for i in range(1, len(data) + 1):\n",
    "        start_time = time.time()\n",
    "        root = insert_bst_iterative(root, data[i - 1])\n",
    "        add_time = time.time() - start_time\n",
    "        add_times.append(add_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        search_bst(root, data[i - 1])\n",
    "        search_time = time.time() - start_time\n",
    "        search_times.append(search_time)\n",
    "\n",
    "    return {\n",
    "        \"add_time\": add_times,\n",
    "        \"search_time\": search_times,\n",
    "        \"memory_usage\": memory_usage_psutil()\n",
    "    }\n",
    "\n",
    "class AVLNode:\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.height = 1\n",
    "\n",
    "def insert_avl(root, key):\n",
    "    if not root:\n",
    "        return AVLNode(key)\n",
    "    elif key < root.key:\n",
    "        root.left = insert_avl(root.left, key)\n",
    "    else:\n",
    "        root.right = insert_avl(root.right, key)\n",
    "\n",
    "    root.height = 1 + max(get_height(root.left), get_height(root.right))\n",
    "    balance = get_balance(root)\n",
    "\n",
    "    if balance > 1 and key < root.left.key:\n",
    "        return right_rotate(root)\n",
    "    if balance < -1 and key > root.right.key:\n",
    "        return left_rotate(root)\n",
    "    if balance > 1 and key > root.left.key:\n",
    "        root.left = left_rotate(root.left)\n",
    "        return right_rotate(root)\n",
    "    if balance < -1 and key < root.right.key:\n",
    "        root.right = right_rotate(root.right)\n",
    "        return left_rotate(root)\n",
    "\n",
    "    return root\n",
    "\n",
    "def left_rotate(z):\n",
    "    if z.right is None:\n",
    "        return z\n",
    "    y = z.right\n",
    "    T2 = y.left\n",
    "    y.left = z\n",
    "    z.right = T2\n",
    "    z.height = 1 + max(get_height(z.left), get_height(z.right))\n",
    "    y.height = 1 + max(get_height(y.left), get_height(y.right))\n",
    "    return y\n",
    "\n",
    "def right_rotate(z):\n",
    "    if z.left is None:\n",
    "        return z\n",
    "    y = z.left\n",
    "    T3 = y.right\n",
    "    y.right = z\n",
    "    z.left = T3\n",
    "    z.height = 1 + max(get_height(z.left), get_height(z.right))\n",
    "    y.height = 1 + max(get_height(y.left), get_height(y.right))\n",
    "    return y\n",
    "\n",
    "def get_height(root):\n",
    "    if not root:\n",
    "        return 0\n",
    "    return root.height\n",
    "\n",
    "def get_balance(root):\n",
    "    if not root:\n",
    "        return 0\n",
    "    return get_height(root.left) - get_height(root.right)\n",
    "\n",
    "def search_avl(root, key):\n",
    "    if not root or root.key == key:\n",
    "        return root\n",
    "    if key < root.key:\n",
    "        return search_avl(root.left, key)\n",
    "    return search_avl(root.right)\n",
    "\n",
    "def benchmark_avl_tree(data):\n",
    "    random.shuffle(data)  # Randomize the data before insertion\n",
    "\n",
    "    add_times = []\n",
    "    search_times = []\n",
    "\n",
    "    root = None\n",
    "\n",
    "    for i in range(1, len(data) + 1):\n",
    "        start_time = time.time()\n",
    "        root = insert_avl(root, data[i - 1])\n",
    "        add_time = time.time() - start_time\n",
    "        add_times.append(add_time)\n",
    "\n",
    "        start_time = time.time()\n",
    "        search_avl(root, data[i - 1])\n",
    "        search_time = time.time() - start_time\n",
    "        search_times.append(search_time)\n",
    "\n",
    "    return {\n",
    "        \"add_time\": add_times,\n",
    "        \"search_time\": search_times,\n",
    "        \"memory_usage\": memory_usage_psutil()\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    capacity = 10000\n",
    "    error_rate = 0.01\n",
    "    num_elements = 10000\n",
    "\n",
    "    data_files = {\n",
    "        \"Natural Language Words\": \"data/natural_language_words.txt\",\n",
    "        \"Random Strings\": \"data/random_strings.txt\",\n",
    "        \"DNA Sequences\": \"data/dna_sequences.txt\",\n",
    "        \"Email Addresses\": \"data/email_addresses.txt\"\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for data_type, file_path in data_files.items():\n",
    "        data = load_data(file_path)\n",
    "        results[data_type] = {\n",
    "            \"bloom_filter\": {},\n",
    "            \"linear_search\": benchmark_linear_search(data),\n",
    "            \"bst\": benchmark_bst(data),\n",
    "            \"avl_tree\": benchmark_avl_tree(data)\n",
    "        }\n",
    "        print(\n",
    "            f\"Benchmarking Bloom Filter with {data_type} (capacity {capacity}, error rate {error_rate}, {num_elements} elements).\"\n",
    "        )\n",
    "        for hash_function in ['jenkins', 'murmur', 'city', 'sha256']:\n",
    "            results[data_type][\"bloom_filter\"][hash_function] = benchmark_bloom_filter(\n",
    "                data, capacity, error_rate, hash_function)\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "    with open(\"results/benchmark_results.json\", \"w\") as file:\n",
    "        json.dump(results, file, indent=4)\n",
    "\n",
    "    print(\"Benchmarking completed and results saved to results/benchmark_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Plotting\n",
    "\n",
    "We plot the results of our benchmarking to visualize the performance metrics of the Bloom Filter and compare it against other data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Results\n",
    "\n",
    "def plot_bloom_filter_metrics(results, data_type, metric, y_label):\n",
    "    grouped_data = {}\n",
    "    for hash_function in results[data_type][\"bloom_filter\"]:\n",
    "        values = results[data_type][\"bloom_filter\"][hash_function][metric]\n",
    "        grouped_data[hash_function] = [values[i:i + GROUP_SIZE] for i in range(0, len(values), GROUP_SIZE)]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for hash_function, groups in grouped_data.items():\n",
    "        mean_values = [np.mean(group) for group in groups]\n",
    "        x_positions = range(0, len(mean_values) * GROUP_SIZE, GROUP_SIZE)\n",
    "\n",
    "        plt.plot(x_positions, mean_values, marker='o', linestyle='-', label=f\"{hash_function}\")\n",
    "\n",
    "    plt.title(f\"{metric.replace('_', ' ').title()} for Bloom Filters ({data_type})\")\n",
    "    plt.xlabel(\"Number of Elements\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"results/{data_type}_bloom_filter_{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_algorithm_metrics(results, data_type, metric, y_label):\n",
    "    grouped_data = {\n",
    "        \"linear_search\": [results[data_type][\"linear_search\"][metric][i:i + GROUP_SIZE] for i in range(0, len(results[data_type][\"linear_search\"][metric]), GROUP_SIZE)],\n",
    "        \"bst\": [results[data_type][\"bst\"][metric][i:i + GROUP_SIZE] for i in range(0, len(results[data_type][\"bst\"][metric]), GROUP_SIZE)],\n",
    "        \"avl_tree\": [results[data_type][\"avl_tree\"][metric][i:i + GROUP_SIZE] for i in range(0, len(results[data_type][\"avl_tree\"][metric]), GROUP_SIZE)]\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for algorithm, groups in grouped_data.items():\n",
    "        mean_values = [np.mean(group) for group in groups]\n",
    "        x_positions = range(0, len(mean_values) * GROUP_SIZE, GROUP_SIZE)\n",
    "\n",
    "        plt.plot(x_positions, mean_values, marker='o', linestyle='-', label=f\"{algorithm}\")\n",
    "\n",
    "    plt.title(f\"{metric.replace('_', ' ').title()} for Algorithms ({data_type})\")\n",
    "    plt.xlabel(\"Number of Elements\")\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"results/{data_type}_algorithms_{metric}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_false_positive_prob(results):\n",
    "    for data_type in results:\n",
    "        grouped_data = {}\n",
    "        for hash_function in results[data_type][\"bloom_filter\"]:\n",
    "            false_positive_rate = results[data_type][\"bloom_filter\"][hash_function][\"false_positive_rate\"]\n",
    "            key = f\"{data_type} ({hash_function})\"\n",
    "            grouped_data[key] = [false_positive_rate[i:i + GROUP_SIZE] for i in range(0, len(false_positive_rate), GROUP_SIZE)]\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "\n",
    "        for label, groups in grouped_data.items():\n",
    "            mean_values = [np.mean(group) for group in groups]\n",
    "            x_positions = range(0, len(mean_values) * GROUP_SIZE, GROUP_SIZE)\n",
    "\n",
    "            plt.plot(x_positions, mean_values, marker='o', linestyle='-', label=label)\n",
    "\n",
    "        plt.title(f\"False Positive Probability (FPP) in Relation to Number of Elements Added ({data_type})\")\n",
    "        plt.xlabel(\"Number of Elements Added\")\n",
    "        plt.ylabel(\"False Positive Probability (FPP)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(f\"results/false_positive_prob_elements_{data_type}.png\")\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    with open(\"results/benchmark_results.json\", \"r\") as file:\n",
    "        results = json.load(file)\n",
    "\n",
    "    for data_type in results:\n",
    "        plot_bloom_filter_metrics(results, data_type, \"add_time\", \"Add Time (s)\")\n",
    "        plot_bloom_filter_metrics(results, data_type, \"check_time\", \"Check Time (s)\")\n",
    "        plot_bloom_filter_metrics(results, data_type, \"false_positive_rate\", \"False Positive Rate\")\n",
    "        plot_algorithm_metrics(results, data_type, \"add_time\", \"Add Time (s)\")\n",
    "        plot_algorithm_metrics(results, data_type, \"search_time\", \"Search Time (s)\")\n",
    "\n",
    "    plot_false_positive_prob(results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis for Questions 7 and 8\n",
    "\n",
    "We perform a detailed analysis to check how the false positive rate changes as the number of elements in the Bloom Filter increases and how the compression rate varies with different expected and observed false positive rates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis for Questions 7 and 8\n",
    "\n",
    "def analyze_false_positive_rate(data, capacity, error_rate, hash_function='jenkins'):\n",
    "    bf = BloomFilter(capacity, error_rate, hash_function)\n",
    "\n",
    "    false_positive_rates = []\n",
    "    false_positives = 0\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        bf.add(item)\n",
    "        if bf.contains(random_string()):\n",
    "            false_positives += 1\n",
    "\n",
    "        false_positive_rate = false_positives / (i + 1)\n",
    "        false_positive_rates.append(false_positive_rate)\n",
    "\n",
    "    return false_positive_rates\n",
    "\n",
    "def analyze_compression_rate(data, capacities, error_rates, hash_function='jenkins'):\n",
    "    results = []\n",
    "\n",
    "    for capacity in capacities:\n",
    "        for error_rate in error_rates:\n",
    "            bf = BloomFilter(capacity, error_rate, hash_function)\n",
    "            for item in data:\n",
    "                bf.add(item)\n",
    "\n",
    "            actual_bit_usage = sum(bin(x).count('1') for x in bf.bitset.bitset)\n",
    "            compression_rate = actual_bit_usage / bf.bitset.size\n",
    "\n",
    "            false_positives = 0\n",
    "            for _ in range(10000):  # Test with 10,000 random strings\n",
    "                if bf.contains(random_string()):\n",
    "                    false_positives += 1\n",
    "\n",
    "            observed_false_positive_rate = false_positives / 10000\n",
    "\n",
    "            results.append({\n",
    "                'capacity': capacity,\n",
    "                'error_rate': error_rate,\n",
    "                'compression_rate': compression_rate,\n",
    "                'observed_false_positive_rate': observed_false_positive_rate\n",
    "            })\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_false_positive_rate(results, title):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label, rates in results.items():\n",
    "        plt.plot(rates, label=label)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Number of Elements Added\")\n",
    "    plt.ylabel(\"False Positive Rate\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"results/{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_compression_rate(results, title, x_key, y_key):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for result in results:\n",
    "        plt.scatter(result[x_key], result[y_key], label=f\"Capacity: {result['capacity']}, Error Rate: {result['error_rate']}\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_key.replace('_', ' ').title())\n",
    "    plt.ylabel(y_key.replace('_', ' ').title())\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"results/{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.close()\n",
    "\n",
    "def main():\n",
    "    capacity = 10000\n",
    "    error_rate = 0.01\n",
    "    num_elements = 15000  # Exceed the designed capacity to test false positive rate changes\n",
    "\n",
    "    data_files = {\n",
    "        \"Natural Language Words\": \"data/natural_language_words.txt\",\n",
    "        \"Random Strings\": \"data/random_strings.txt\",\n",
    "        \"DNA Sequences\": \"data/dna_sequences.txt\",\n",
    "        \"Email Addresses\": \"data/email_addresses.txt\"\n",
    "    }\n",
    "\n",
    "    results_q7 = {}\n",
    "    results_q8 = []\n",
    "\n",
    "    os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "    for data_type, file_path in data_files.items():\n",
    "        data = load_data(file_path)[:num_elements]  # Limit data to num_elements\n",
    "\n",
    "        # Question 7: False Positive Rate Analysis\n",
    "        results_q7[data_type] = analyze_false_positive_rate(data, capacity, error_rate, hash_function='jenkins')\n",
    "\n",
    "        # Question 8: Compression Rate Analysis\n",
    "        capacities = [5000, 10000, 20000]\n",
    "        error_rates = [0.001, 0.01]\n",
    "        results_q8.extend(analyze_compression_rate(data, capacities, error_rates, hash_function='jenkins'))\n",
    "\n",
    "    # Plot results for Question 7\n",
    "    plot_false_positive_rate(results_q7, \"False Positive Rate Over Time\")\n",
    "\n",
    "    # Plot results for Question 8\n",
    "    plot_compression_rate(results_q8, \"Compression Rate vs Expected False Positives\", 'error_rate', 'compression_rate')\n",
    "    plot_compression_rate(results_q8, \"Compression Rate vs Observed False Positives\", 'observed_false_positive_rate', 'compression_rate')\n",
    "\n",
    "    # Save results to JSON for further inspection\n",
    "    with open(\"results/q7_results.json\", \"w\") as file:\n",
    "        json.dump(results_q7, file, indent=4)\n",
    "\n",
    "    with open(\"results/q8_results.json\", \"w\") as file:\n",
    "        json.dump(results_q8, file, indent=4)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
